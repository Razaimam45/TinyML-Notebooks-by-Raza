{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantizing a deep modelâ€™s weights from FP32 (32-bit floating point) to a bit-width of b. We have implemented asymmetric quantization and symmetric quantization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint 'https://hanlab18.mit.edu/files/course/labs/vgg.cifar.pretrained.pth'\n",
      "Original model size: 35.20 MB\n",
      "\n",
      "Quantizing to 8 bits:\n",
      "Symmetric quantization:\n",
      "Quantized model size: 8.80 MB\n",
      "Compression ratio: 4.00x\n",
      "Average quantization MSE: 0.000001\n",
      "\n",
      "Asymmetric quantization:\n",
      "Quantized model size: 8.80 MB\n",
      "Compression ratio: 4.00x\n",
      "Average quantization MSE: 0.000000\n",
      "\n",
      "Quantizing to 4 bits:\n",
      "Symmetric quantization:\n",
      "Quantized model size: 4.40 MB\n",
      "Compression ratio: 8.00x\n",
      "Average quantization MSE: 0.000220\n",
      "\n",
      "Asymmetric quantization:\n",
      "Quantized model size: 4.40 MB\n",
      "Compression ratio: 8.00x\n",
      "Average quantization MSE: 0.000077\n",
      "\n",
      "Quantizing to 2 bits:\n",
      "Symmetric quantization:\n",
      "Quantized model size: 2.20 MB\n",
      "Compression ratio: 16.00x\n",
      "Average quantization MSE: 0.010571\n",
      "\n",
      "Asymmetric quantization:\n",
      "Quantized model size: 2.20 MB\n",
      "Compression ratio: 16.00x\n",
      "Average quantization MSE: 0.001350\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "def download_url(url):\n",
    "    filename = url.split('/')[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Downloading {url}\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    return filename\n",
    "\n",
    "class VGG(nn.Module):\n",
    "  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n",
    "\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "\n",
    "    layers = []\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    def add(name: str, layer: nn.Module) -> None:\n",
    "      layers.append((f\"{name}{counts[name]}\", layer))\n",
    "      counts[name] += 1\n",
    "\n",
    "    in_channels = 3\n",
    "    for x in self.ARCH:\n",
    "      if x != 'M':\n",
    "        # conv-bn-relu\n",
    "        add(\"conv\", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))\n",
    "        add(\"bn\", nn.BatchNorm2d(x))\n",
    "        add(\"relu\", nn.ReLU(True))\n",
    "        in_channels = x\n",
    "      else:\n",
    "        # maxpool\n",
    "        add(\"pool\", nn.MaxPool2d(2))\n",
    "    add(\"avgpool\", nn.AvgPool2d(2))\n",
    "    self.backbone = nn.Sequential(OrderedDict(layers))\n",
    "    self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]\n",
    "    x = self.backbone(x)\n",
    "\n",
    "    # avgpool: [N, 512, 2, 2] => [N, 512]\n",
    "    # x = x.mean([2, 3])\n",
    "    x = x.view(x.shape[0], -1)\n",
    "\n",
    "    # classifier: [N, 512] => [N, 10]\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "\n",
    "def symmetric_quantize_layer(weight, bit_width):\n",
    "    \"\"\"\n",
    "    Symmetrically quantize a layer's weights\n",
    "    \"\"\"\n",
    "    n_levels = 2 ** (bit_width - 1) - 1\n",
    "    scale = torch.max(torch.abs(weight)) / n_levels\n",
    "    \n",
    "    # Quantize weights\n",
    "    quantized_weight = torch.clip(torch.round(weight / scale), -n_levels, n_levels)\n",
    "    \n",
    "    # Dequantize\n",
    "    dequantized_weight = quantized_weight * scale\n",
    "    return dequantized_weight, scale\n",
    "\n",
    "def asymmetric_quantize_layer(weight, bit_width):\n",
    "    \"\"\"\n",
    "    Asymmetrically quantize a layer's weights\n",
    "    \"\"\"\n",
    "    n_levels = 2 ** bit_width - 1\n",
    "    w_max = torch.max(weight)\n",
    "    w_min = torch.min(weight)\n",
    "    \n",
    "    scale = (w_max - w_min) / n_levels\n",
    "    zero_point = torch.round(-w_min / scale)\n",
    "    \n",
    "    # Quantize weights\n",
    "    quantized_weight = torch.clip(torch.round(weight / scale + zero_point), 0, n_levels)\n",
    "    \n",
    "    # Dequantize\n",
    "    dequantized_weight = (quantized_weight - zero_point) * scale\n",
    "    return dequantized_weight, scale, zero_point\n",
    "\n",
    "def quantize_model(model, bit_width, quantization_type='symmetric'):\n",
    "    \"\"\"\n",
    "    Quantize all weights in the model\n",
    "    \"\"\"\n",
    "    quantized_state_dict = {}\n",
    "    scaling_factors = {}\n",
    "    zero_points = {}\n",
    "    \n",
    "    # Quantize each parameter in the model\n",
    "    for name, param in model.state_dict().items():\n",
    "        if 'weight' in name:  # Only quantize weights, not biases\n",
    "            if quantization_type == 'symmetric':\n",
    "                quantized_weight, scale = symmetric_quantize_layer(param.data, bit_width)\n",
    "                quantized_state_dict[name] = quantized_weight\n",
    "                scaling_factors[name] = scale\n",
    "            else:  # asymmetric\n",
    "                quantized_weight, scale, zero_point = asymmetric_quantize_layer(param.data, bit_width)\n",
    "                quantized_state_dict[name] = quantized_weight\n",
    "                scaling_factors[name] = scale\n",
    "                zero_points[name] = zero_point\n",
    "        else:\n",
    "            quantized_state_dict[name] = param.data\n",
    "            \n",
    "    return quantized_state_dict, scaling_factors, zero_points\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model's accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load model\n",
    "    checkpoint_url = \"https://hanlab18.mit.edu/files/course/labs/vgg.cifar.pretrained.pth\"\n",
    "    model = VGG().to(device)\n",
    "    checkpoint = torch.load(download_url(checkpoint_url), map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    print(f\"=> loaded checkpoint '{checkpoint_url}'\")\n",
    "    \n",
    "    # Original model size\n",
    "    original_size = sum(p.numel() * 32 for p in model.parameters()) / 8  # in bytes\n",
    "    print(f\"Original model size: {original_size / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Quantize model (try both symmetric and asymmetric)\n",
    "    bit_widths = [8, 4, 2]  # Test different bit widths\n",
    "    \n",
    "    for bit_width in bit_widths:\n",
    "        print(f\"\\nQuantizing to {bit_width} bits:\")\n",
    "        \n",
    "        # Symmetric quantization\n",
    "        quantized_state_dict, scaling_factors, _ = quantize_model(model, bit_width, 'symmetric')\n",
    "        \n",
    "        # Create a new model instance for the quantized weights\n",
    "        quantized_model = VGG().to(device)\n",
    "        quantized_model.load_state_dict(quantized_state_dict)\n",
    "        \n",
    "        # Calculate quantized model size\n",
    "        quantized_size = sum(p.numel() * bit_width for p in quantized_model.parameters()) / 8  # in bytes\n",
    "        \n",
    "        print(f\"Symmetric quantization:\")\n",
    "        print(f\"Quantized model size: {quantized_size / 1024 / 1024:.2f} MB\")\n",
    "        print(f\"Compression ratio: {original_size / quantized_size:.2f}x\")\n",
    "        \n",
    "        # Calculate quantization error\n",
    "        error = 0\n",
    "        total_params = 0\n",
    "        for (name, orig_param), (_, quant_param) in zip(model.named_parameters(), \n",
    "                                                       quantized_model.named_parameters()):\n",
    "            if 'weight' in name:\n",
    "                error += torch.mean((orig_param - quant_param) ** 2).item()\n",
    "                total_params += 1\n",
    "        \n",
    "        print(f\"Average quantization MSE: {error/total_params:.6f}\")\n",
    "        \n",
    "        # Do the same for asymmetric quantization\n",
    "        quantized_state_dict, scaling_factors, zero_points = quantize_model(model, bit_width, 'asymmetric')\n",
    "        quantized_model = VGG().to(device)\n",
    "        quantized_model.load_state_dict(quantized_state_dict)\n",
    "        \n",
    "        print(f\"\\nAsymmetric quantization:\")\n",
    "        print(f\"Quantized model size: {quantized_size / 1024 / 1024:.2f} MB\")\n",
    "        print(f\"Compression ratio: {original_size / quantized_size:.2f}x\")\n",
    "        \n",
    "        error = 0\n",
    "        for (name, orig_param), (_, quant_param) in zip(model.named_parameters(), \n",
    "                                                       quantized_model.named_parameters()):\n",
    "            if 'weight' in name:\n",
    "                error += torch.mean((orig_param - quant_param) ** 2).item()\n",
    "        \n",
    "        print(f\"Average quantization MSE: {error/total_params:.6f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sigprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
