{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c57f0c",
   "metadata": {},
   "source": [
    "# Lab-1 (1): Self-Attention and Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb34b0b",
   "metadata": {},
   "source": [
    "**Background:** A basic transformer block $\\mathcal{F}$ consists of a Multi-head Self-Attention (MHSA), Layer Normalization (LN), Feed-forward Network (FFN), and Residual Connections (RC). It can be formulated as:\n",
    "\\begin{equation} \n",
    "\\begin{aligned}\\mathbf{z}_{\\ell}^{\\prime}=\\text{MHSA}\\left(\\text{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1};          \n",
    "               \\mathbf{z}_{\\ell}=\\text{FFN}\\left(\\text{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}; \n",
    "               i.e., \\mathbf{z}_{\\ell} = \\mathcal{F}_{\\ell-1}(\\mathbf{z}_{\\ell-1})\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where $\\mathbf{z}_{\\ell}^{\\prime}$ and $\\mathbf{z}_{\\ell-1}$ are the intermediate representations. $\\mathcal{F}_{\\ell}$ indicates the transformer block at $\\ell$-th layer. $\\ell \\in \\{0, 1,\\dots, L\\}$ is the layer index and $L$ is the number of hidden layers. The self-attention module is realized by the inner products with a scaling factor and a {\\em softmax} operation, which is written as:\n",
    "\\begin{equation}\n",
    "\\operatorname{Attention}(Q, K, V)=\\operatorname{Softmax}\\left(Q K^{\\top}/{\\sqrt{d_{k}}}\\right) V\n",
    "\\end{equation}\n",
    "where $Q, K, V$ are ${\\textit{query}}$, ${\\textit{key}}$ and ${\\textit{value}}$ vectors, respectively. $1/\\sqrt{d_k}$ is the scaling factor for normalization. Multi-head self attention further concatenates the parallel attention layers to increase the representation ability:\n",
    "\\begin{equation}\n",
    "\\text{MHSA}(Q, K, V)=\\operatorname{Concat}\\left(\\operatorname{head}_{1}, \\ldots,  \\operatorname{ head }_{\\mathrm{h}}\\right) W^{O}, \n",
    "\\end{equation}\n",
    "where $W^{O} \\in \\mathbb{R}^{h d_{v} \\times d_{\\text {model }}}$. $\\text{head}_{\\mathrm{i}}=\\operatorname{Attention}\\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\\right)$ are the projections with parameter matrices $W_{i}^{Q} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{k}}, W_{i}^{K} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{k}}, W_{i}^{V} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{v}}$.\n",
    "\n",
    "The FFN contains two linear layers with a GELU non-linearity in between\n",
    "\\begin{equation}\n",
    "\\mathrm{FFN}(x)= (\\text{GELU}\\left(\\mathbf{z} W_{1}+b_{1}\\right) ) W_{2}+b_{2}\n",
    "\\end{equation}\n",
    "where $\\mathbf{z}$ is the input. $W_{1},b_{1},W_{2},b_{2}$ are the two linear layers' weights and biases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0134fe13",
   "metadata": {},
   "source": [
    "**Task 1:** Complete the code of MLP module in Transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa66b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a3102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: two fully connected layers with actiation and dropout\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1433e",
   "metadata": {},
   "source": [
    "**Task 2:** Complete the code of multi-head self-attention module in Transformer block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ec8948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape # Batch, token lengt/patches, Dimension/features\n",
    "        # YOUR CODE HERE\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        print(\"\\nInitial q,k,v shapes\", q.shape, k.shape, v.shape)\n",
    "        q = q.reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k = k.reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = v.reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        print(\"\\nReshaped q,k,v shapes\", q.shape, k.shape, v.shape)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        print(\"\\nInitial attn shape\", attn.shape)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        print(\"\\nSoftmaxed attn shape\", attn.shape)\n",
    "        attn = self.attn_drop(attn)\n",
    "        print(\"\\nDropped attn shape\", attn.shape)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        print(\"\\nAfter multiplying attn and v\", x.shape)\n",
    "        x = self.proj(x)\n",
    "        print(\"\\nAfter projection\", x.shape)\n",
    "        x = self.proj_drop(x)\n",
    "        print(\"\\nAfter dropping\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25873cc3",
   "metadata": {},
   "source": [
    "**Task 3:** Complete the code for building a Transformer block using multi-head self-attnetion and MLP modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ecc8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        print(\"\\n---After attention\", x.shape)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        print(\"\\n---After mlp\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791bda5f",
   "metadata": {},
   "source": [
    "**Test the code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d16b084f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======>Input Before Self-Attention: \n",
      " torch.Size([2, 4, 32])\n",
      "\n",
      "Initial q,k,v shapes torch.Size([2, 4, 32]) torch.Size([2, 4, 32]) torch.Size([2, 4, 32])\n",
      "\n",
      "Reshaped q,k,v shapes torch.Size([2, 8, 4, 4]) torch.Size([2, 8, 4, 4]) torch.Size([2, 8, 4, 4])\n",
      "\n",
      "Initial attn shape torch.Size([2, 8, 4, 4])\n",
      "\n",
      "Softmaxed attn shape torch.Size([2, 8, 4, 4])\n",
      "\n",
      "Dropped attn shape torch.Size([2, 8, 4, 4])\n",
      "\n",
      "After multiplying attn and v torch.Size([2, 4, 32])\n",
      "\n",
      "After projection torch.Size([2, 4, 32])\n",
      "\n",
      "After dropping torch.Size([2, 4, 32])\n",
      "\n",
      "---After attention torch.Size([2, 4, 32])\n",
      "\n",
      "---After mlp torch.Size([2, 4, 32])\n",
      "\n",
      "======>Output After Self-Attention: \n",
      "torch.Size([2, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE\n",
    "model = Transformer_Block(dim=32, num_heads=8)\n",
    "\n",
    "before_self_attention = torch.randn(2, 4, 32) # Batch, N_len, C_dim\n",
    "print(\"======>Input Before Self-Attention: \\n\", before_self_attention.shape)\n",
    "\n",
    "attn_forward = model(before_self_attention)\n",
    "print(\"\\n======>Output After Self-Attention: \")\n",
    "print(attn_forward.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
