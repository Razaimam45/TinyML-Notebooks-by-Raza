{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 9: Mixture Of Experts (MoE)\n",
    "In this lab, you will learn how to implement and train a simple Mixture of Experts (MoE) model using a single GPU. The model based on the `Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer` (https://arxiv.org/abs/1701.06538)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseDispatcher(object):\n",
    "    \"\"\"Helper for implementing a mixture of experts.\n",
    "    The purpose of this class is to create input minibatches for the\n",
    "    experts and to combine the results of the experts to form a unified\n",
    "    output tensor.\n",
    "    There are two functions:\n",
    "    dispatch - take an input Tensor and create input Tensors for each expert.\n",
    "    combine - take output Tensors from each expert and form a combined output\n",
    "      Tensor.  Outputs from different experts for the same batch element are\n",
    "      summed together, weighted by the provided \"gates\".\n",
    "    The class is initialized with a \"gates\" Tensor, which specifies which\n",
    "    batch elements go to which experts, and the weights to use when combining\n",
    "    the outputs.  Batch element b is sent to expert e iff gates[b, e] != 0.\n",
    "    The inputs and outputs are all two-dimensional [batch, depth].\n",
    "    Caller is responsible for collapsing additional dimensions prior to\n",
    "    calling this class and reshaping the output to the original shape.\n",
    "    See common_layers.reshape_like().\n",
    "    Example use:\n",
    "    gates: a float32 `Tensor` with shape `[batch_size, num_experts]`\n",
    "    inputs: a float32 `Tensor` with shape `[batch_size, input_size]`\n",
    "    experts: a list of length `num_experts` containing sub-networks.\n",
    "    dispatcher = SparseDispatcher(num_experts, gates)\n",
    "    expert_inputs = dispatcher.dispatch(inputs)\n",
    "    expert_outputs = [experts[i](expert_inputs[i]) for i in range(num_experts)]\n",
    "    outputs = dispatcher.combine(expert_outputs)\n",
    "    The preceding code sets the output for a particular example b to:\n",
    "    output[b] = Sum_i(gates[b, i] * experts[i](inputs[b]))\n",
    "    This class takes advantage of sparsity in the gate matrix by including in the\n",
    "    `Tensor`s for expert i only the batch elements for which `gates[b, i] > 0`.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_experts, gates):\n",
    "        self._gates = gates\n",
    "        self._num_experts = num_experts\n",
    "        sorted_experts, index_sorted_experts = torch.nonzero(gates).sort(0)\n",
    "        _, self._expert_index = sorted_experts.split(1, dim=1)\n",
    "        self._batch_index = torch.nonzero(gates)[index_sorted_experts[:, 1], 0]\n",
    "        self._part_sizes = (gates > 0).sum(0).tolist()\n",
    "        gates_exp = gates[self._batch_index.flatten()]\n",
    "        self._nonzero_gates = torch.gather(gates_exp, 1, self._expert_index)\n",
    "\n",
    "    def dispatch(self, inp):\n",
    "        \"\"\"The `Tensor` for a expert `i` contains the slices of `inp` corresponding\n",
    "        to the batch elements `b` where `gates[b, i] > 0`.\n",
    "        Args:\n",
    "          inp: a `Tensor` of shape \"[batch_size, <extra_input_dims>]`\n",
    "        Returns:\n",
    "          a list of `num_experts` `Tensor`s with shapes\n",
    "            `[expert_batch_size_i, <extra_input_dims>]`.\n",
    "        \"\"\"\n",
    "        inp_exp = inp[self._batch_index].squeeze(1)\n",
    "        return torch.split(inp_exp, self._part_sizes, dim=0)\n",
    "\n",
    "    def combine(self, expert_out, multiply_by_gates=True):\n",
    "        \"\"\"Sum together the expert output, weighted by the gates.\n",
    "        The slice corresponding to a particular batch element `b` is computed\n",
    "        as the sum over all experts `i` of the expert output, weighted by the\n",
    "        corresponding gate values.  If `multiply_by_gates` is set to False, the\n",
    "        gate values are ignored.\n",
    "        Args:\n",
    "          expert_out: a list of `num_experts` `Tensor`s, each with shape\n",
    "            `[expert_batch_size_i, <extra_output_dims>]`.\n",
    "          multiply_by_gates: a boolean\n",
    "        Returns:\n",
    "          a `Tensor` with shape `[batch_size, <extra_output_dims>]`.\n",
    "        \"\"\"\n",
    "        stitched = torch.cat(expert_out, 0)\n",
    "\n",
    "        if multiply_by_gates:\n",
    "            stitched = stitched.mul(self._nonzero_gates)\n",
    "        zeros = torch.zeros(self._gates.size(0), expert_out[-1].size(1), requires_grad=True, device=stitched.device)\n",
    "        combined = zeros.index_add(0, self._batch_index, stitched.float())\n",
    "        return combined\n",
    "\n",
    "    def expert_to_gates(self):\n",
    "        \"\"\"Gate values corresponding to the examples in the per-expert `Tensor`s.\n",
    "        Returns:\n",
    "          a list of `num_experts` one-dimensional `Tensor`s with type `tf.float32`\n",
    "              and shapes `[expert_batch_size_i]`\n",
    "        \"\"\"\n",
    "        return torch.split(self._nonzero_gates, self._part_sizes, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.soft = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.soft(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoE(nn.Module):\n",
    "    \"\"\"Call a Sparsely gated mixture of experts layer with 1-layer Feed-Forward networks as experts.\n",
    "    Args:\n",
    "    input_size: integer - size of the input\n",
    "    output_size: integer - size of the input\n",
    "    num_experts: an integer - number of experts\n",
    "    hidden_size: an integer - hidden size of the experts\n",
    "    noisy_gating: a boolean\n",
    "    k: an integer - how many experts to use for each batch element\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, num_experts, hidden_size, noisy_gating=True, k=4):\n",
    "        super(MoE, self).__init__()\n",
    "        self.noisy_gating = noisy_gating\n",
    "        self.num_experts = num_experts\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.k = k\n",
    "        self.experts = nn.ModuleList([MLP(self.input_size, self.output_size, self.hidden_size) for i in range(self.num_experts)])\n",
    "        self.w_gate = nn.Parameter(torch.zeros(input_size, num_experts), requires_grad=True)\n",
    "        self.w_noise = nn.Parameter(torch.zeros(input_size, num_experts), requires_grad=True)\n",
    "\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.softmax = nn.Softmax(1)\n",
    "        self.register_buffer(\"mean\", torch.tensor([0.0]))\n",
    "        self.register_buffer(\"std\", torch.tensor([1.0]))\n",
    "        assert(self.k <= self.num_experts)\n",
    "\n",
    "    def cv_squared(self, x):\n",
    "        \"\"\"The squared coefficient of variation of a sample.\n",
    "        Useful as a loss to encourage a positive distribution to be more uniform.\n",
    "        Epsilons added for numerical stability.\n",
    "        Returns 0 for an empty Tensor.\n",
    "        Args:\n",
    "        x: a `Tensor`.\n",
    "        Returns:\n",
    "        a `Scalar`.\n",
    "        \"\"\"\n",
    "        eps = 1e-10\n",
    "        if x.shape[0] == 1:\n",
    "            return torch.tensor([0], device=x.device, dtype=x.dtype)\n",
    "        return x.float().var() / (x.float().mean()**2 + eps)\n",
    "\n",
    "    def _gates_to_load(self, gates):\n",
    "        \"\"\"Compute the true load per expert, given the gates.\n",
    "        The load is the number of examples for which the corresponding gate is >0.\n",
    "        Args:\n",
    "        gates: a `Tensor` of shape [batch_size, n]\n",
    "        Returns:\n",
    "        a float32 `Tensor` of shape [n]\n",
    "        \"\"\"\n",
    "        return (gates > 0).sum(0)\n",
    "\n",
    "    def _prob_in_top_k(self, clean_values, noisy_values, noise_stddev, noisy_top_values):\n",
    "        \"\"\"Helper function to NoisyTopKGating.\n",
    "        Computes the probability that value is in top k, given different random noise.\n",
    "        This gives us a way of backpropagating from a loss that balances the number\n",
    "        of times each expert is in the top k experts per example.\n",
    "        In the case of no noise, pass in None for noise_stddev, and the result will\n",
    "        not be differentiable.\n",
    "        Args:\n",
    "        clean_values: a `Tensor` of shape [batch, n].\n",
    "        noisy_values: a `Tensor` of shape [batch, n].  Equal to clean values plus\n",
    "          normally distributed noise with standard deviation noise_stddev.\n",
    "        noise_stddev: a `Tensor` of shape [batch, n], or None\n",
    "        noisy_top_values: a `Tensor` of shape [batch, m].\n",
    "           \"values\" Output of tf.top_k(noisy_top_values, m).  m >= k+1\n",
    "        Returns:\n",
    "        a `Tensor` of shape [batch, n].\n",
    "        \"\"\"\n",
    "        batch = clean_values.size(0)\n",
    "        m = noisy_top_values.size(1)\n",
    "        top_values_flat = noisy_top_values.flatten()\n",
    "        threshold_positions_if_in = torch.arange(batch, device=clean_values.device) * m + self.k\n",
    "        threshold_if_in = torch.unsqueeze(torch.gather(top_values_flat, 0, threshold_positions_if_in), 1)\n",
    "        is_in = torch.gt(noisy_values, threshold_if_in)\n",
    "        threshold_positions_if_out = threshold_positions_if_in - 1\n",
    "        threshold_if_out = torch.unsqueeze(torch.gather(top_values_flat, 0, threshold_positions_if_out), 1)\n",
    "        normal = Normal(self.mean, self.std)\n",
    "        prob_if_in = normal.cdf((clean_values - threshold_if_in)/noise_stddev)\n",
    "        prob_if_out = normal.cdf((clean_values - threshold_if_out)/noise_stddev)\n",
    "        prob = torch.where(is_in, prob_if_in, prob_if_out)\n",
    "        return prob\n",
    "\n",
    "    def noisy_top_k_gating(self, x, train, noise_epsilon=1e-2):\n",
    "        \"\"\"Noisy top-k gating.\n",
    "          See paper: https://arxiv.org/abs/1701.06538.\n",
    "          Args:\n",
    "            x: input Tensor with shape [batch_size, input_size]\n",
    "            train: a boolean - we only add noise at training time.\n",
    "            noise_epsilon: a float\n",
    "          Returns:\n",
    "            gates: a Tensor with shape [batch_size, num_experts]\n",
    "            load: a Tensor with shape [num_experts]\n",
    "        \"\"\"\n",
    "        clean_logits = x @ self.w_gate\n",
    "        if self.noisy_gating and train:\n",
    "            raw_noise_stddev = x @ self.w_noise\n",
    "            noise_stddev = ((self.softplus(raw_noise_stddev) + noise_epsilon))\n",
    "            noisy_logits = clean_logits + (torch.randn_like(clean_logits) * noise_stddev)\n",
    "            logits = noisy_logits\n",
    "        else:\n",
    "            logits = clean_logits\n",
    "        logits = self.softmax(logits)\n",
    "        top_logits, top_indices = logits.topk(min(self.k + 1, self.num_experts), dim=1)\n",
    "        top_k_logits = top_logits[:, :self.k]\n",
    "        top_k_indices = top_indices[:, :self.k]\n",
    "        top_k_gates = top_k_logits / (top_k_logits.sum(1, keepdim=True) + 1e-6)  # normalization\n",
    "\n",
    "        zeros = torch.zeros_like(logits, requires_grad=True)\n",
    "        gates = zeros.scatter(1, top_k_indices, top_k_gates)\n",
    "\n",
    "        if self.noisy_gating and self.k < self.num_experts and train:\n",
    "            load = (self._prob_in_top_k(clean_logits, noisy_logits, noise_stddev, top_logits)).sum(0)\n",
    "        else:\n",
    "            load = self._gates_to_load(gates)\n",
    "        return gates, load\n",
    "\n",
    "    def forward(self, x, loss_coef=1e-2):\n",
    "        \"\"\"Args:\n",
    "        x: tensor shape [batch_size, input_size]\n",
    "        train: a boolean scalar.\n",
    "        loss_coef: a scalar - multiplier on load-balancing losses\n",
    "\n",
    "        Returns:\n",
    "        y: a tensor with shape [batch_size, output_size].\n",
    "        extra_training_loss: a scalar.  This should be added into the overall\n",
    "        training loss of the model.  The backpropagation of this loss\n",
    "        encourages all experts to be approximately equally used across a batch.\n",
    "        \"\"\"\n",
    "        ############### YOUR CODE STARTS HERE ###############\n",
    "        #use the noisy_top_k_gating function to get the gates and load\n",
    "        gates, load = self.noisy_top_k_gating(x, self.training)\n",
    "        # sum the values of the gates to get the importance\n",
    "        importance = gates.sum(0)\n",
    "        # calculate the loss\n",
    "        loss = loss_coef * (self.cv_squared(importance) + self.cv_squared(load))\n",
    "        loss *= loss_coef\n",
    "        # use the SparseDispatcher to dispatch the input to the experts\n",
    "        dispatcher = SparseDispatcher(self.num_experts, gates)\n",
    "        # get the expert inputs\n",
    "        expert_inputs = dispatcher.dispatch(x)\n",
    "        # send the expert inputs to the experts\n",
    "        gates = gates.unsqueeze(2)\n",
    "        # get the expert outputs\n",
    "        expert_outputs = [self.experts[i](expert_inputs[i]) for i in range(self.num_experts)]\n",
    "        # combine the expert outputs\n",
    "        y = dispatcher.combine(expert_outputs)\n",
    "        ############### YOUR CODE ENDS HERE ###############\n",
    "        return y, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[1,   100] loss: 2.175\n",
      "[1,   200] loss: 2.117\n",
      "[1,   300] loss: 2.100\n",
      "[1,   400] loss: 2.107\n",
      "[1,   500] loss: 2.085\n",
      "[1,   600] loss: 2.084\n",
      "[1,   700] loss: 2.089\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 38 %\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "net = MoE(input_size=3072, output_size=10, num_experts=10, hidden_size=128, noisy_gating=True, k=4)\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "net.train()\n",
    "for epoch in range(1):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.view(inputs.shape[0], -1)\n",
    "        outputs, aux_loss = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss = loss + aux_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    \n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "correct = 0\n",
    "total = 0\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs, _ = net(images.view(images.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
